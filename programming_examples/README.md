<!-- This file is auto-generated by generate_readme.py. Do not edit manually. -->

# MLIR-AIR Programming Examples

These programming examples demonstrate how to leverage the AIR design flow with mlir-air Python bindings and the mlir-air intermediate representation (IR) to build applications targeting AI Engines on AMD NPUs.

## Operator Dashboard

| Category | Operation | Datatype(s) | NPU1 | NPU2 | Design Example |
|:---------|:----------|:------------|:----:|:----:|:---------------|
| Linear Algebra | [Matrix Multiplication](matrix_multiplication/) | bf16, i16, i8 | ðŸŸ¢ | ðŸŸ¢ | [matrix_multiplication/](matrix_multiplication/) |
| Linear Algebra | [Vector-Matrix Multiplication](vector_matrix_multiplication/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [vector_matrix_multiplication/](vector_matrix_multiplication/) |
| Linear Algebra | [AXPY](axpy/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [axpy/](axpy/) |
| Element-wise | [Element-wise Add](eltwise_add/) | f32 | ðŸŸ¢ | ðŸŸ¢ | [eltwise_add/](eltwise_add/) |
| Element-wise | [Element-wise Add (with L2)](eltwise_add_with_l2/) | f32 | ðŸŸ¢ | ðŸŸ¢ | [eltwise_add_with_l2/](eltwise_add_with_l2/) |
| Activation/Math | [Softmax](softmax/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [softmax/](softmax/) |
| Activation/Math | [Sine / Cosine](sine_cosine/) | bf16 | ðŸŸ¢ | âšª | [sine_cosine/](sine_cosine/) |
| Activation/Math | [RELU](relu/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [relu/](relu/) |
| Activation/Math | [Leaky RELU](leaky_relu/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [leaky_relu/](leaky_relu/) |
| Normalization | [Layer Normalization](layer_norm/) | bf16 | âšª | ðŸŸ¢ | [layer_norm/](layer_norm/) |
| Normalization | [RMS Normalization](rms_norm/) | bf16 | âšª | ðŸŸ¢ | [rms_norm/](rms_norm/) |
| LLM Kernels | [Multi-Head Attention (LLaMA2)](llama2_mha/) | bf16 | ðŸŸ¢ | âšª | [llama2_mha/](llama2_mha/) |
| LLM Kernels | [RoPE (LLaMA2)](llama2_rope/) | bf16 | ðŸŸ¢ | âšª | [llama2_rope/](llama2_rope/) |
| Attention | [Flash Attention (Dataflow)](flash_attention/dataflow_based/) | bf16 | ðŸŸ¢ | âšª | [flash_attention/dataflow_based/](flash_attention/dataflow_based/) |
| Attention | [Flash Attention (Kernel Fusion)](flash_attention/kernel_fusion_based/) | bf16 | âšª | ðŸŸ¢ | [flash_attention/kernel_fusion_based/](flash_attention/kernel_fusion_based/) |
| Data Movement | [Passthrough (DMA)](passthrough/passthrough_dma/) | u8 | ðŸŸ¢ | ðŸŸ¢ | [passthrough/passthrough_dma/](passthrough/passthrough_dma/) |
| Data Movement | [Passthrough (Channel)](passthrough/passthrough_channel/) | u8 | ðŸŸ¢ | ðŸŸ¢ | [passthrough/passthrough_channel/](passthrough/passthrough_channel/) |
| Data Movement | [Passthrough (Kernel)](passthrough/passthrough_kernel/) | u8 | ðŸŸ¢ | ðŸŸ¢ | [passthrough/passthrough_kernel/](passthrough/passthrough_kernel/) |
| Data Movement | [Shim DMA 2D](shim_dma_2d/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [shim_dma_2d/](shim_dma_2d/) |
| Data Movement | [Data Transfer Transpose](data_transfer_transpose/) | u32 | ðŸŸ¢ | ðŸŸ¢ | [data_transfer_transpose/](data_transfer_transpose/) |
| Data Movement | [Matrix Scalar Add](matrix_scalar_add/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [matrix_scalar_add/](matrix_scalar_add/) |
| Communication | [Channel Examples](channel_examples/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [channel_examples/](channel_examples/) |
| Communication | [Multi-Segment Examples](multi_segment/) | i32 | ðŸŸ¡ | ðŸŸ¡ | [multi_segment/](multi_segment/) |
| Communication | [Cascade Reduction](cascade_reduction/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [cascade_reduction/](cascade_reduction/) |
| Memory | [Segment Alloc](segment_alloc/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [segment_alloc/](segment_alloc/) |
| Spatial | [Segment Unroll](segment_unroll/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [segment_unroll/](segment_unroll/) |
| Dataflow | [Herd Dataflow](herd_dataflow/) | bf16 | ðŸŸ¢ | ðŸŸ¢ | [herd_dataflow/](herd_dataflow/) |
| Control Flow | [Conditional Branching](conditional_branching/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [conditional_branching/](conditional_branching/) |
| CNN | [2D Convolution](conv2d/) | i32 | ðŸŸ¢ | ðŸŸ¢ | [conv2d/](conv2d/) |
| CNN | [Bottleneck](bottleneck/) | bf16 | ðŸŸ¢ | âšª | [bottleneck/](bottleneck/) |
| Memory | [Shared L1 Buffer](shared_l1/) | bf16 | ðŸŸ¢ | âšª | [shared_l1/](shared_l1/) |
| Primitives | [Scalar/Vector Operations](primitives/) | various | ðŸŸ¢ | ðŸŸ¢ | [primitives/](primitives/) |

### Status Legend

- ðŸŸ¢ Supported and tested
- ðŸŸ¡ Work in progress
- âšª Not yet supported

**NPU1** = AMD Ryzen AI (Phoenix, AIE2) &nbsp;&nbsp; **NPU2** = AMD Ryzen AI (Strix, AIE2P)

## Getting Started

See the top-level [README](../README.md) for environment setup and build instructions. Once your environment is configured:

```bash
# Example: run matrix multiplication (bf16, 4x4 herd, 512x512x512)
cd matrix_multiplication/bf16
make run4x4

# Print generated MLIR without running
make print
```

Most examples with a `Makefile` support `make run` (compile and execute on hardware) and `make print` (generate MLIR only). Examples without a Makefile can be run directly with Python:

```bash
python3 run.py                    # compile and run (XRTRunner)
python3 run.py --print-module-only  # print IR only
```

## Benchmarking

The [matrix multiplication](matrix_multiplication/) examples include sweep infrastructure for measuring end-to-end latency across problem sizes:

```bash
cd matrix_multiplication/bf16
make sweep4x4    # sweep problem sizes 256-2048 with a 4x4 herd
make profile     # profile a single 1024^3 problem on hardware
```

Sweep results are saved as CSV files for analysis. See the [bf16 README](matrix_multiplication/bf16/README.md) for details on tile size configuration and architecture selection.
